{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73fd093-84b1-48f3-897b-7e3e5cc733a0",
   "metadata": {},
   "source": [
    "#### Student Name: Mai Ngo\n",
    "#### Course Name and Number: DSC 430 Python Programming\n",
    "#### Assignment Name and Number: Assignment0602_SurfCDM\n",
    "#### Date: 2/15/2023\n",
    "#### Honor Statement: â€œI have not given or received any unauthorized assistance on this assignment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a62d9d-48a3-4bb0-a4fd-ac55476cb14f",
   "metadata": {},
   "source": [
    "#### Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ee2d9-7aae-43b8-bc6d-0b3b92a58f94",
   "metadata": {},
   "source": [
    "I put a lot of commands '#' after each code, because I'm learning while doing this homework. Make note to understand what each line means specifically. \\\n",
    "Minor check to make sure I'm doing it right:\n",
    "\n",
    "    *URL: ' https://depaulmagazine.com/2022/12/08/professional-partners/\" has leading whitespace. \n",
    "    *NOTE: Just CDM homepage: 187 count '<a' tag | 173 count URL links | 9 '<a' tags in footer | 5 '<a' tags don't start with 'http'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9becb8d-63ba-426d-bd5a-4b3508fb080e",
   "metadata": {},
   "source": [
    "#### Show how you extended HTMLParser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0113a-9547-4768-8555-d22f430a3389",
   "metadata": {},
   "source": [
    "I coded handle_starttag() to get all hyperlinks correctly. \\\n",
    "For handle_data, I converted all data to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df59f27-c1cc-4ed3-b8d2-e30e668d6f26",
   "metadata": {},
   "source": [
    "#### Show which methods you overwrote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad5cc7-0890-4509-8fa7-175c6ef5dad2",
   "metadata": {},
   "source": [
    "I created new def cleanWord() to remove stop word, and remove special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387b4d2-bc12-4086-bc25-bea84993fbc0",
   "metadata": {},
   "source": [
    "#### Show how you restricted your search to webpages at CDM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238c3da-4f72-4df2-b65d-34c1fd76ce31",
   "metadata": {},
   "source": [
    "I overwrote analyze()- to only get hyper links start with base link 'https://www.cdm.depaul.edu/'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22c9622-e0bc-4fb6-afc5-eb8af0d1dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urljoin, urlparse, Request\n",
    "from html.parser import HTMLParser\n",
    "import collections\n",
    "from collections import Counter\n",
    "#Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "83128140-2cdc-4738-bc24-2a3bd5834a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(HTMLParser):\n",
    "    \"\"\"Collects hyperlink URLs into a list\"\"\"\n",
    "\n",
    "    ## (*) For our purpose, we want to ignore several tags that prevents\n",
    "    ## extraction of clean text.  We define them as a class variable.\n",
    "    ignore_tags = ['script', 'noscript', 'input', 'meta', 'title', 'style', 'form']\n",
    "    tag = ''\n",
    "    def __init__(self, url):\n",
    "        \"\"\" initializes parser, the url, and a list \"\"\"\n",
    "        HTMLParser.__init__(self)\n",
    "        self.url = url\n",
    "        self.links = []\n",
    "        self.data = []\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \"\"\" collects hyperlink URLs in their absolute format \"\"\"      \n",
    "        self.tag = tag\n",
    "        #Assign tag to an instance variable.\n",
    "        if self.tag == 'a':\n",
    "        #<a> tag defines a hyperlink\n",
    "        \n",
    "            for attr in attrs:\n",
    "            #Iterate over each piece of HTML. \n",
    "                if attr[0] == 'href':\n",
    "                #Specifies a URL.  \n",
    "                \n",
    "                    absolute = urljoin(self.url, attr[1].strip())\n",
    "                    #Assign absolute variable: Construct absolute URL, join base URL and path attr[1]. \n",
    "                    #Using strip() because ' https://depaulmagazine.com/2022/12/08/professional-partners/' has leading whitespace \n",
    "                    \n",
    "                    if absolute[:4] == 'http': # collect HTTP URLs\n",
    "                        self.links.append(absolute)\n",
    "                   \n",
    "    def handle_data(self, data):\n",
    "        if self.tag not in self.ignore_tags:\n",
    "            for word in data.strip().split():\n",
    "                gen_word = word.lower()\n",
    "                self.data.append(gen_word)\n",
    "\n",
    "    def getLinks(self):\n",
    "        \"\"\" returns hyperlinks URLs in their absolute format \"\"\"\n",
    "        return self.links\n",
    "\n",
    "    def getData(self):\n",
    "        \"\"\" returns the data (accumulated in the instance variable) \"\"\"\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4872d1b5-4f2b-4165-8026-4e6ed3044ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(url):\n",
    "    '''Returns list of all http links in url, in absolute format.'''\n",
    "    \n",
    "    print('\\n\\nVisiting', url)           \n",
    "    #Print the initial visiting url - CDM Website\n",
    "\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\"\n",
    "    #Assign crawler agent to avoid HTTP Error 418\n",
    "    request = Request(url)\n",
    "    #Send HTTP request using the URL\n",
    "    \n",
    "    request.add_header(\"User-Agent\",user_agent)\n",
    "    #Tells the web server what kind of operating system and browser: (Mozilla/5.0)\n",
    "    content = urlopen(request).read().decode()\n",
    "    #Makes a HTTP transaction to retrieve the associated resource.\n",
    "    \n",
    "    collector = Collector(url)\n",
    "    collector.feed(content)\n",
    "    #Feed a string containing HTML, processes it\n",
    "    urls = collector.getLinks()\n",
    "    \n",
    "    url_s = []\n",
    "    #Collect URL within CDM.\n",
    "    for url in urls:\n",
    "    #Iterate over each url from getLinks()\n",
    "        if url[:27] == 'https://www.cdm.depaul.edu/':\n",
    "        #If the hyperlink has the same base link.\n",
    "            url_s.append(url)   \n",
    "            #Then append.\n",
    "        else: pass\n",
    "    \n",
    "    content = collector.getData()\n",
    "    #Retrieve content, have converted to all lower case. \n",
    "    cleandata = cleanWord(content)\n",
    "    #Clean data once again, special chars, stop words.\n",
    "    \n",
    "    word_count = frequency(cleandata)\n",
    "    #Give frequency of each word in content.\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6152aac8-777c-444e-b55c-7394a5665958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanWord(content):\n",
    "    '''To remove stop word, and remove special characters.'''\n",
    "   \n",
    "    filename = \"C:/Users/maimu/OneDrive/Documents/DePaul/DSC 430//M6_stopwords.txt\"\n",
    "    stopWords = []\n",
    "    with open(filename) as infile:\n",
    "    #Open stop word file and convert into a list of word.\n",
    "        for word in infile: \n",
    "            word = word.strip() \n",
    "            stopWords.append(word)\n",
    "    \n",
    "    cleanWordlst = []\n",
    "    for word in content:\n",
    "    #Iterate through each word.\n",
    "    \n",
    "        symbols = \"!@#$%^&*()_-+={[}]|\\;:\\\"<>?/., \"\n",
    "        #Potential symbols in content.\n",
    "        for i in range(len(symbols)):\n",
    "        #Iterate over each symbol.\n",
    "            word = word.replace(symbols[i], '')\n",
    "            #Replace special character with nothing.\n",
    "            \n",
    "        if len(word) > 1:\n",
    "        #If word length is greater than 1, which meanns not speical character. \n",
    "            cleanWordlst.append(word)\n",
    "            #Append the word to final clean content.\n",
    "        \n",
    "    cleanWordlst2 = []\n",
    "    #Finbal clean content.\n",
    "    for word2 in cleanWordlst:\n",
    "    #Iterate through wordlist after remove all special characters. \n",
    "    \n",
    "        if word2 not in stopWords:\n",
    "        #If the word is not in stop words list.\n",
    "            cleanWordlst2.append(word2)\n",
    "                \n",
    "    return cleanWordlst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49a65c52-dd28-4e64-bc41-f0137af40162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(content):\n",
    "    '''Return count of each word.'''\n",
    "    #sentence is a list of words.\n",
    "    \n",
    "    word_count = Counter(content)\n",
    "    #Return each word's count.\n",
    "    \n",
    "    wordCount = {}\n",
    "    #Assign wordCount variable to store count, as dictionary.\n",
    "    \n",
    "    for wordlst in content:\n",
    "    #Iterate over each word-component in 'sentence' list.\n",
    "    #wordlst variable is a string.\n",
    "    \n",
    "        words = wordlst.strip().split()\n",
    "        #Assign variable 'words' splits 'wordlst' string into a list 'words'\n",
    "        for word in words:\n",
    "            #Iterate over each word in 'words' list.\n",
    "            gen_word = word.lower()\n",
    "            if word in wordCount:\n",
    "            #Check if the word is in already in wordCount dictionary.\n",
    "                wordCount[word] += 1\n",
    "                #If so, add one more count to the respective word.\n",
    "                \n",
    "            else:\n",
    "            #If the word is not in the wordCount dictionary.\n",
    "                wordCount[word] = 1\n",
    "                #Adding new word with count as 1.\n",
    "    \n",
    "    return word_count\n",
    "    #return word_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d60d5bc9-2e6a-4057-92da-847d0457d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = set() \n",
    "#Initialize an empty set 'visited'. Each stored link will be unique. \n",
    "\n",
    "def crawl(url):\n",
    "    ''' Recursive web crawler that calls analyze() on every visited web page.'''\n",
    "    \n",
    "    global visited\n",
    "    #Declare 'visited' throughout the programme. Links already in there will not be visited again.\n",
    "    visited.add(url)\n",
    "    #Add url link to 'visited' set after crawl.\n",
    "\n",
    "\n",
    "    links = analyze(url)\n",
    "    #Assign variable 'links', returns a list of URL hyperlinks in visited web page.\n",
    "\n",
    "    for link in links:\n",
    "    #Iterate over each URL link in 'links' list\n",
    "        if link not in visited:\n",
    "        #If link has not been visited yet. \n",
    "        \n",
    "            try: crawl(link)\n",
    "            #Execute using def crawl(), which using def analyze()\n",
    "            #Recursively continue crawl from every link in links.\n",
    "            \n",
    "            except: pass\n",
    "            #In case link is already visited. Pass to next link.\n",
    "            \n",
    "        else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1a364203-4b40-4033-a8c0-0fb3aab653b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printOutput(wordCount):\n",
    "    '''Test and write palindrome dates to a file.'''\n",
    "    #Given a list of reversed year numbers (4 digits), and a list of date (2 digits). \n",
    "    \n",
    "    outfile = open('HW0602.txt', 'w', encoding = \"utf-8\")\n",
    "    #Create/Open a text file named 'HW0602' to write only.\n",
    "    outfile.write('The most 50 common words on the CDM website: \\n')\n",
    "    #Give header\n",
    "        \n",
    "    common = wordCount.most_common(50)\n",
    "    for word1, count1 in common:\n",
    "            outfile.write('{:20} {:5} \\n'.format(word1, count1))\n",
    "            \n",
    "    outfile.close()\n",
    "    #Close the file at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4005687-d3a3-4660-b932-dd49400c7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = 'https://www.cdm.depaul.edu/'\n",
    "    word_count = analyze(url)\n",
    "    wordCount = frequency(word_count)\n",
    "    printOutput(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1032c14c-94ea-4267-a720-62c6445f54e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visiting https://www.cdm.depaul.edu/\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c6bb6-09d4-4ffd-b93c-63c0e17bcb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
